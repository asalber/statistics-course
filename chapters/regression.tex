% Author: Alfredo Sánchez Alberca (asalber@gmail.com)
% !TEX main = statistics_manual.tex
\section{Regression and correlation}

\mode<presentation>{
%---------------------------------------------------------------------slide----
\begin{frame}
\frametitle{Regression and Correlation}
\tableofcontents[sectionstyle=show/hide,hideothersubsections]
\end{frame}
}


% ---------------------------------------------------------------------slide----
\begin{frame}
\frametitle{Relations among variables}
In the last chapter we saw how to describe the distribution of a single variable in a sample. 
However, in most cases, studies require to describe several variables that are often related.
For instance, a nutritional study should consider all the variables that could be related to the weight, as height, age, gender, smoking, diet, physic exercise, etc.

To understand a phenomenon that involve several variables is not enough to study every variable by its own. 
We have to study all the variables together to describe how they interact and the type of relation among them. 

Usually in a \emph{dependency study} there is a \highlight{dependent variable} $Y$ that it is supposed to be influenced
by a set of variables $X_1,\ldots,X_n$ known as \highlight{independent variables}. 
The simpler case is a \emph{simple dependency study} when there is only one independent variable, that is the case covered in this chapter. 
\end{frame}


\subsection{Joint frequency distribution}

% ---------------------------------------------------------------------slide----
\begin{frame}
\frametitle{Joint frequencies}
To study the relation between two variables $X$ and $Y$, we have to study the joint distribution of the \highlight{two-dimensional variable} $(X,Y)$, whose values are pairs $(x_i,y_j)$ where the first element is a value of $X$ and the second a value of $Y$.

\begin{definition}[Joint sample frequencies]
Given a sample of $n$ values and a two-dimensional variable $(X,Y)$, for every value of the variable $(x_i,y_j)$ is defined
\begin{itemize}
\item \highlight{Absolute frequency $n_{ij}$}: Is the number of times that the pair $(x_i,y_j)$ appears in the sample.
\item \highlight{Relative frequency $f_{ij}$}: Is the proportion of times that the pair $(x_i,y_j)$ appears in the
sample.
\[
f_{ij}=\frac{n_{ij}}{n}
\]
\end{itemize}
\end{definition}

\begin{center}
\alert{\emph{Watch out! For two-dimensional variables it make no sense cumulative frequencies.}}
\end{center}
\end{frame}


%---------------------------------------------------------------------slide----
\begin{frame}
\frametitle{Joint frequency distribution}
The values of the two-dimensional variable with their frequencies is known as \highlight{joint frequency distribution}, and is represented in a \highlight{joint frequency table}.
\[
\begin{array}{|c|ccccc|}
\hline
X\backslash Y & y_1 & \cdots & y_j & \cdots & y_q\\
\hline
x_1 & n_{11} & \cdots & n_{1j} & \cdots & n_{1q}\\
\vdots & \vdots & \vdots & \vdots & \vdots & \vdots\\
x_i & n_{i1} & \cdots & n_{ij} & \cdots & n_{iq}\\
\vdots & \vdots & \vdots & \vdots & \vdots & \vdots\\
x_p & n_{p1} & \cdots & n_{pj} & \cdots & n_{pq}\\
\hline
\end{array}
\]
\end{frame}


%---------------------------------------------------------------------slide----
\begin{frame}
\frametitle{Joint frequency distribution}
\framesubtitle{Example with grouped data}
The height (in cm) and weight (in kg) of a sample of 30 students is:
\begin{center}
(179,85), (173,65), (181,71), (170,65), (158,51), (174,66),\\
(172,62), (166,60), (194,90), (185,75), (162,55), (187,78),\\
(198,109), (177,61), (178,70), (165,58), (154,50), (183,93),\\
(166,51), (171,65), (175,70), (182,60), (167,59), (169,62),\\
(172,70), (186,71), (172,54), (176,68),(168,67), (187,80).
\end{center}

The joint frequency table is 
\[
\begin{array}{|c||c|c|c|c|c|c|}
\hline
  X/Y & [50,60) & [60,70) & [70,80) & [80,90) & [90,100) & [100,110) \\
  \hline\hline
  (150,160] & 2 & 0 & 0 & 0 & 0 & 0 \\
  \hline
  (160,170] & 4 & 4 & 0 & 0 & 0 & 0 \\
  \hline
  (170,180] & 1 & 6 & 3 & 1 & 0 & 0 \\
  \hline
  (180,190] & 0 & 1 & 4 & 1 & 1 & 0 \\
  \hline
  (190,200] & 0 & 0 & 0 & 0 & 1 & 1 \\
  \hline
\end{array}
\]
\end{frame}


%---------------------------------------------------------------------slide----
\begin{frame}
\frametitle{Scatter plot}
The joint frequency distribution can be represented graphically with a \highlight{scatter plot}, where data is displayed as a collection of points on a $XY$ coordinate system.

Usually the independent variable is represented in the $X$ axis and the dependent variable in the $Y$ axis.
For every data pair $(x_i,y_j)$ in the sample a dot is drawn on the plane with those coordinates.  
\begin{center}
\tikzsetnextfilename{regression/scatterplot}
\resizebox{0.4\textwidth}{!}{\input{img/regression/scatterplot}}
\end{center}

The result is a set of points that usually is known as a \emph{point cloud}.

%A scatter plot only represent the observed values in the sample but not their frequencies.
%To represent frequencies there are other types of charts like \emph{buble charts} or \emph{three-dimensional
% histograms}.

%\begin{center}
%\alert{\emph{¡Watch out! It make no sense for qualitative variables.}}
%\end{center}
\end{frame}


%---------------------------------------------------------------------slide----
\begin{frame}
\frametitle{Scatter plot}
\framesubtitle{Example of heights and weights}
\begin{center}
\tikzsetnextfilename{regression/height_weight_scatterplot}
\mode<article>{\resizebox{0.7\textwidth}{!}{\input{img/regression/height_weight_scatterplot}}}
\mode<presentation>{\resizebox{0.9\textwidth}{!}{\input{img/regression/height_weight_scatterplot}}}
\end{center}
\end{frame}


%---------------------------------------------------------------------slide----
\begin{frame}
\frametitle{Scatter plot interpretation}
The shape of the point cloud in a scatter plot gives information about the type of relation between the variables.

\tikzsetnextfilename{regression/scatterplot_different_relations}
\resizebox{\textwidth}{!}{\input{img/regression/scatterplot_different_relations}}
\end{frame}


%---------------------------------------------------------------------slide----
\begin{frame}
\frametitle{Marginal frequency distributions}
The frequency distributions of each variable of the two-dimensional variable are known as \highlight{marginal frequency distributions}.

We can get the marginal frequency distributions from the joint frequency table by adding frequencies by rows and columns. 

\begin{center}
\[
\begin{array}{|c|ccccc|>{\columncolor{color1!50}}c|}
\hline
X\backslash Y & y_1 & \cdots & y_j & \cdots & y_q & n_x\\
\hline
x_1 & n_{11} & \cdots & n_{1j} & \cdots & n_{1q} & n_{x_1}\\
\vdots & \vdots & \vdots & \tikz[baseline=-0.5ex]{\node at (0,0) [fill=color2!50,single arrow,shape border
rotate=270, single arrow head extend=1mm]{$+$\phantom{}}; } & \vdots &
\vdots & \vdots \\
x_i & n_{i1} & \tikz[baseline=-0.5ex]{\node at (0,0) [fill=color1!50,single arrow,shape border rotate=0,
single arrow head extend=1mm]{$+$\phantom{}}; }  & n_{ij} & \tikz[baseline=-0.5ex]{\node
at (0,0) [fill=color1!50,single arrow,shape border rotate=0, single
arrow head extend=1mm]{$+$\phantom{}}; } & n_{iq} & n_{x_i}\\
\vdots & \vdots & \vdots & \tikz[baseline=-0.5ex]{\node at (0,0) [fill=color2!50,single arrow,shape border
rotate=270, single arrow head extend=1mm]{$+$\phantom{}}; } & \vdots & \vdots & \vdots\\
x_p & n_{p1} & \cdots & n_{pj} & \cdots & n_{pq} & n_{x_p} \\
\hline
\rowcolor{color2!50}
n_y & n_{y_1} & \cdots & n_{y_j} & \cdots & n_{y_q} & \cellcolor{white} n\\
\hline
\end{array}
\]
\end{center}
\end{frame}


%---------------------------------------------------------------------slide----
\begin{frame}
\frametitle{Marginal frequency distributions}
\framesubtitle{Example of heights and weights}
The marginal frequency distributions for the previous sample of heights and weights are
\[
\begin{array}{|c||c|c|c|c|c|c|>{\columncolor{color1!50}}c|}
\hline
  X/Y & [50,60) & [60,70) & [70,80) & [80,90) & [90,100) & [100,110) & n_x\\
  \hline\hline
  (150,160] & 2 & 0 & 0 & 0 & 0 & 0 & 2\\
  \hline
  (160,170] & 4 & 4 & 0 & 0 & 0 & 0 & 8\\
  \hline
  (170,180] & 1 & 6 & 3 & 1 & 0 & 0 & 11 \\
  \hline
  (180,190] & 0 & 1 & 4 & 1 & 1 & 0 & 7 \\
  \hline
  (190,200] & 0 & 0 & 0 & 0 & 1 & 1 & 2\\
  \hline
  \rowcolor{color2!50}
  n_y & 7 & 11 & 7 & 2 & 2 & 1 & \cellcolor{white} 30\\
  \hline
\end{array}
\]
and the corresponding statistics are
\[
\begin{array}{lllll}
\bar x = 174.67 \mbox{ cm} & \quad & s^2_x = 102.06 \mbox{ cm}^2 & \quad & s_x = 10.1 \mbox{ cm}\\
\bar y = 69.67 \mbox{ Kg} & & s^2_y = 164.42 \mbox{ Kg}^2 & & s_y = 12.82 \mbox{ Kg}
\end{array}
\]
\end{frame}


\subsection{Covariance}
%---------------------------------------------------------------------slide----
\begin{frame}
\frametitle{Deviations from the means}
To study the relation between two variables, we have to analyze the joint variation of them.
\begin{center}
\tikzsetnextfilename{regression/deviations_to_means}
\mode<article>{\resizebox{0.7\textwidth}{!}{\input{img/regression/deviations_to_means}}}
\mode<presentation>{\resizebox{0.8\textwidth}{!}{\input{img/regression/deviations_to_means}}}
\end{center}
\end{frame}


%---------------------------------------------------------------------slide----
\begin{frame}
\frametitle{Sign of deviations from the mean}
Dividing the point cloud of the scatter plot in 4 quadrants centered in the mean point $(\bar x, \bar y)$, the sign of
deviations from the mean is:
\begin{center}
%\mode<presentation>{\rowcolors{1}{red!20}{red!10}}
\begin{tabular}{cccc}
Quadrant & $(x_i-\bar x)$ & $(y_j-\bar y)$ & $(x_i-\bar x)(y_j-\bar y)$\\
\hline
1 & $+$ & $+$ & \alert{$+$}\\
2 & $-$ & $+$ & \alert{$-$}\\
3 & $-$ & $-$ & \alert{$+$}\\
4 & $+$ & $-$ & \alert{$-$}\\
\hline
\end{tabular}

\tikzsetnextfilename{regression/scatterplot_quadrants}
\resizebox{0.4\textwidth}{!}{\input{img/regression/scatterplot_quadrants}}
\end{center}
\end{frame}


%---------------------------------------------------------------------slide----
\begin{frame}
\frametitle{Sign of the product of deviations from the mean}
\begin{columns}[t]
\begin{column}{0.5\textwidth}
If there is an \emph{increasing linear} relationship between the variables, most of the points will fall in quadrants 1 and 3, and the sum of the products of deviations from the mean will be positive.

\centering
\tikzsetnextfilename{regression/increasing_linear_scatterplot}
\mode<article>{\resizebox{0.6\textwidth}{!}{\input{img/regression/increasing_linear_scatterplot}}}
\mode<presentation>{\resizebox{0.9\textwidth}{!}{\input{img/regression/increasing_linear_scatterplot}}}

\[\sum(x_i-\bar x)(y_j-\bar y) >0\]
\end{column}

\begin{column}{0.5\textwidth}
If there is an \emph{decreasing linear} relationship between the variables, most of the points will fall in quadrants 2
and 4, and the sum of the products of deviations from the mean will be negative.

\centering
\tikzsetnextfilename{regression/decreasing_linear_scatterplot}
\mode<article>{\resizebox{0.6\textwidth}{!}{\input{img/regression/decreasing_linear_scatterplot}}}
\mode<presentation>{\resizebox{0.9\textwidth}{!}{\input{img/regression/decreasing_linear_scatterplot}}}

\[\sum(x_i-\bar x)(y_j-\bar y) <0\]
\end{column}
\end{columns}
\end{frame}


%---------------------------------------------------------------------slide----
\begin{frame}
\frametitle{Covariance}
Using the products of deviations from the means we get the statistic
\begin{definition}[Sample covariance]
The \emph{sample covariance} of a two-dimensional variable $(X,Y)$ is the average of the products of deviations from the respective means.
\[
s_{xy}=\frac{\sum (x_i-\bar x)(y_j-\bar y)n_{ij}}{n}
\]
\end{definition}
It can also be calculated using the formula
\[
s_{xy}=\frac{\sum x_iy_jn_{ij}}{n}-\bar x\bar y.
\]

The covariance measures the linear relation between two variables:
\begin{itemize}
\item If $s_{xy}>0$ there exists an increasing linear relation.
\item If $s_{xy}<0$ there exists a decreasing linear relation.
\item If $s_{xy}=0$ there is no linear relation.
\end{itemize}
\end{frame}


%---------------------------------------------------------------------slide----
\begin{frame}
\frametitle{Covariance calculation}
\framesubtitle{Example of heights and weights}
Using the joint frequency table of the sample of heights and weights
\[
\begin{array}{|c||c|c|c|c|c|c|c|}
\hline
  X/Y & [50,60) & [60,70) & [70,80) & [80,90) & [90,100) & [100,110) & n_x\\
  \hline\hline
  (150,160] & 2 & 0 & 0 & 0 & 0 & 0 & 2\\
  \hline
  (160,170] & 4 & 4 & 0 & 0 & 0 & 0 & 8\\
  \hline
  (170,180] & 1 & 6 & 3 & 1 & 0 & 0 & 11 \\
  \hline
  (180,190] & 0 & 1 & 4 & 1 & 1 & 0 & 7 \\
  \hline
  (190,200] & 0 & 0 & 0 & 0 & 1 & 1 & 2\\
  \hline
  n_y & 7 & 11 & 7 & 2 & 2 & 1 & 30\\
  \hline
\end{array}
\]
\[
\bar x = 174.67 \mbox{ cm} \qquad \bar y = 69.67 \mbox{ Kg}
\]
we get that the covariance is equal to 
\begin{align*}
s_{xy} &=\frac{\sum x_iy_jn_{ij}}{n}-\bar x\bar y =  \frac{155\cdot 55\cdot 2 + 165\cdot 55\cdot 4 + \cdots + 195\cdot 105\cdot 1}{30}-174.67\cdot 69.67 =\\
& = \frac{368200}{30}-12169.26 = 104.07 \mbox{ cm$\cdot$ Kg},
\end{align*}
This means that there is a increasing linear relation between the weight and the height.
\end{frame}


\subsection{Regression}
%---------------------------------------------------------------------slide----
\begin{frame}
\frametitle{Regression}
In most cases the goal of a dependency study is not only to detect a relation between two variables, but also to express that relation with a mathematical function, 
\[y=f(x)\]
in order to predict the dependent variable for every value of the independent one.   

The part of Statistics in charge of constructing such a function is called \highlight{regression}, and the function is known as \highlight{regression function} or \highlight{regression model}.
\end{frame}


%---------------------------------------------------------------------slide----
\begin{frame}
\frametitle{Simple regression models}
There are a lot of types of regression models.
The most common models are shown in the table below.

\begin{center}
%\mode<presentation>{\rowcolors{1}{red!20}{red!10}}
\begin{tabular}{lc}
\toprule
\textbf{Model} & \textbf{Equation}\\
Linear & $y=a+bx$\\
Quadratic & $y=a+bx+cx^2$\\
Cubic & $y=a+bx+cx^2+dx^3$\\
Potential & $y=a\cdot x^b$\\
Exponential & $y=e^{a+bx}$\\
Logarithmic & $y=a+b\log x$\\
Inverse & $y = a+\frac{b}{x}$\\
Sigmoidal & $y= e^{a+\frac{b}{x}}$\\
\bottomrule
\end{tabular}
\end{center}

The model choice depends on the shape of the points cloud in the scatter plot.
\end{frame}


%---------------------------------------------------------------------slide----
\begin{frame}
\frametitle{Residuals or predictive errors}
Once chosen the type of regression model, we have to determine which function of that family explains better the relation between the dependent and the independent variables, that is, the function that predicts better the dependent variable. 

That function is the function that minimizes the distances from the observed values for $Y$ in the sample to the predicted values of the regression function.
These distances are known as \emph{residuals} or \emph{predictive errors}.

\begin{definition}[Residuals or predictive errors]
Given a regression model $y=f(x)$ for a two-dimensional variable $(X,Y)$, the \emph{residual} or \emph{predictive error} for every pair $(x_i,y_j)$ of the sample is the difference between the observed value of the dependent variable $y_j$ and the predicted value of the regression function for $x_i$,
\[
e_{ij} = y_j-f(x_i).
\]
\end{definition}
\end{frame}


%---------------------------------------------------------------------slide----
\begin{frame}
\frametitle{Residuals or predictive errors on $Y$}
\centering
\tikzsetnextfilename{regression/y_residuals}
\mode<article>{\resizebox{0.7\textwidth}{!}{\input{img/regression/y_residuals}}}
\mode<presentation>{\resizebox{0.9\textwidth}{!}{\input{img/regression/y_residuals}}}
\end{frame}


%---------------------------------------------------------------------slide----
\begin{frame}
\frametitle{Least squares fitting}
A way to get the regression function is the \emph{least squares method}, that determines the function that minimizes the squared residuals.
\[
\sum e_{ij}^2.
\]

For a linear model $f(x) = a + bx$, the sum depends on two parameters, the intercept $a$, and the slope $b$ of the straight line,
\[
\theta(a,b) = \sum e_{ij}^2 =\sum (y_j - f(x_i))^2 =\sum (y_j-a-bx_i)^2.
\]

This reduces the problem to determine the values of $a$ and $b$ that minimize this sum. 
\end{frame}


\subsection{Regression line}
%---------------------------------------------------------------------slide----
\begin{frame}
\frametitle{Least squares fitting of a linear model}
To solve the minimization problem, we have to set to zero the partial derivatives with respect to $a$ and $b$.  
\begin{align*}
\frac{\partial \theta(a,b)}{\partial a} &=  \frac{\partial \sum (y_j-a-bx_i)^2 }{\partial a} =0\\
\frac{\partial \theta(a,b)}{\partial b} &=  \frac{\partial \sum (y_j-a-bx_i)^2 }{\partial b} =0
\end{align*}
And solving the equation system, we get
\[
a= \bar y - \frac{s_{xy}}{s_x^2}\bar x \qquad b=\frac{s_{xy}}{s_x^2}
\]

This values minimize the residuals on $Y$ and give us the optimal linear model.
\end{frame}


%---------------------------------------------------------------------slide----
\begin{frame}
\frametitle{Regression line}
\begin{definition}[Regression line]
Given a sample of a two-dimensional variable $(X,Y)$, the \emph{regression line} of $Y$ on $X$ is
\[
y = \bar y +\frac{s_{xy}}{s_x^2}(x-\bar x).
\]
\end{definition}

The regression line of $Y$ on $X$ is the straight line that minimizes the predictive errors on $Y$, therefore it is the linear regression model that gives better predictions of $Y$.
\end{frame}


%---------------------------------------------------------------------slide----
\begin{frame}
\frametitle{Regression line calculation}
\framesubtitle{Example of heights and weights}
Using the previous sample of heights ($X$) and weights ($Y$) with the following statistics
\[
\begin{array}{lllll}
\bar x = 174.67 \mbox{ cm} & \quad & s^2_x = 102.06 \mbox{ cm}^2 & \quad & s_x = 10.1 \mbox{ cm}\\
\bar y = 69.67 \mbox{ Kg} & & s^2_y = 164.42 \mbox{ Kg}^2 & & s_y = 12.82 \mbox{ Kg}\\
& & s_{xy} = 104.07 \mbox{ cm$\cdot$ Kg} & &
\end{array}
\]
the regression line of weight on height is  
\[
y = \bar y +\frac{s_{xy}}{s_x^2}(x-\bar x) = 69.67+\frac{104.07}{102.06}(x-174.67) = -108.49 +1.02 x
\]
And the regression line of height on weight is 
\[
x = \bar x +\frac{s_{xy}}{s_y^2}(y-\bar y) = 174.67+\frac{104.07}{164.42}(y-69.67) = 130.78 + 0.63 y
\]
\begin{center}
\emph{Observe that the regression lines are different!}
\end{center}
\end{frame}


%---------------------------------------------------------------------slide----
\begin{frame}
\frametitle{Regression lines}
\framesubtitle{Example of heights and weights}
\begin{center}
\tikzsetnextfilename{regression/regression_lines}
\mode<article>{\resizebox{0.7\textwidth}{!}{\input{img/regression/regression_lines}}}
\mode<presentation>{\resizebox{0.9\textwidth}{!}{\input{img/regression/regression_lines}}}
\end{center}
\end{frame}


%------------------------------------------ ---------------------------slide----
\begin{frame}
\frametitle{Relative position of the regression lines}
Usually, the regression line of $Y$ on $X$ and the regression line of $X$
on $Y$ are not the same, but they always intersect in the mean point $(\bar x,\bar y)$.
\medskip
\begin{columns}[t]
\begin{column}{0.48\textwidth}
If there is a perfect linear relation between the variables, then both regression lines are the same, as that line makes both $X$-residuals and $Y$-residuals zero.
\begin{center}
\tikzsetnextfilename{regression/perfect_linear_regression}
\mode<article>{\resizebox{0.6\textwidth}{!}{\input{img/regression/perfect_linear_regression}}}
\mode<presentation>{\resizebox{0.9\textwidth}{!}{\input{img/regression/perfect_linear_regression}}}
\end{center}
\end{column}
\begin{column}{0.48\textwidth}
If there is no linear relation between the variables, then both regression lines are constant and equals to the
respective means, 
\begin{center}
$y = \bar y$, \qquad $x = \bar x,$
\end{center}
So, they intersect perpendicularly.
\begin{center}
\tikzsetnextfilename{regression/non_linear_regression}
\mode<article>{\resizebox{0.6\textwidth}{!}{\input{img/regression/non_linear_regression}}}
\mode<presentation>{\resizebox{0.9\textwidth}{!}{\input{img/regression/non_linear_regression}}}
\end{center}
\end{column}
\end{columns}
\end{frame}


%---------------------------------------------------------------------slide----
\begin{frame}
\frametitle{Regression coefficient}
The most important parameter of a regression line is the slope. 

\begin{definition}[Regression coefficient $b_{yx}$]
Given a sample of a two-dimensional variable $(X,Y)$, the \emph{regression coefficient} of the regression line of $Y$ on $X$ is its slope,
\[
b_{yx} = \frac{s_{xy}}{s_x^2}
\]
\end{definition}

The regression coefficient has always the same sign as the covariance. 

It measures how the dependent variable changes in relation to the independent one according to the regression line. 
In particular, it gives the number of units that the dependent variable increases or decreases for every unit that the independent variable increases. 
\end{frame}


%---------------------------------------------------------------------slide----
\begin{frame}
\frametitle{Regression coefficient}
\framesubtitle{Example of heights and weights}
In the sample of heights and weights, the regression line of weight on height was 
\[
y=-108.49 +1.02 x.
\]
Thus, the regression coefficient of weight on height is
\[
b_{yx}= 1.02 \mbox{Kg/cm.}
\]
That means that, according to the regression line of weight on height, the weight will increase $1.02$ Kg for every cm that the height increases. 
\begin{center}
\tikzsetnextfilename{regression/slope_interpretation}
\input{img/regression/slope_interpretation}
\end{center}
\end{frame}

%---------------------------------------------------------------------slide----
\begin{frame}
\frametitle{Regression predictions}
\framesubtitle{Example of heights and weights}
Usually the regression models are used to predict the dependent variable for some values of the independent variable.
\begin{center}
\alert{\emph{Watch out! To get the best predictions of a variable you have to use the regression line where that variable plays the dependent variable role.}}
\end{center}

Thus, in the sample of heights and weights, to predict the weight of a person with a height of 180 cm, we have to use the regression line of weight on height,
\[
y = -108.49 + 1.02 \cdot 180  = 75.11 \mbox{ Kg}.
\]
But to predict the height of a person with a weight of 79 Kg, we have to use the regression line of height on weight, 
\[
x = 130.78 + 0.63\cdot 79 = 180.55 \mbox{ cm}.
\]
\begin{center}
\emph{However, how reliable are these predictions?}
\end{center}
\end{frame}


\subsection{Correlation}
% ---------------------------------------------------------------------slide----
\begin{frame}
\frametitle{Correlation}
Once we have a regression model, in order to see if it is a good predictive model we have to assess the goodness of fit of the model and the strength of the of relation set by it. 
The part of Statistics in charge of this is \highlight{correlation}.

The correlation study the residuals of a regression model: the smaller the residuals, the greater the goodness of fit, and the stronger the relation set by the model. 
\end{frame}


%---------------------------------------------------------------------slide----
\begin{frame}
\frametitle{Residual variance}
To measure the goodness of fit of a regression model is common to use the \emph{residual variance}.
\begin{definition}[Sample residual variance $s_{ry}^2$]
Given a regression model $y=f(x)$ of a two-dimensional variable $(X,Y)$, its \emph{sample residual variance} is the average of the squared residuals,
\[
s_{ry}^2 = \frac{\sum e_{ij}^2n_{ij}}{n} = \frac{\sum (y_j - f(x_i))^2n_{ij}}{n}.
\]
\end{definition}

The greater the residuals, the greater the residual variance and the smaller the goodness of fit. 

When the linear relation is perfect, the residuals are zero and the residual variance is zero. 
Conversely, when there are no relation, the residuals coincide with deviations from the mean, and the residual variance is equal to the variance of the dependent variable.
\[
0\leq s_{ry}^2\leq s_y^2
\]
\end{frame}


%---------------------------------------------------------------------slide----
\begin{frame}
\frametitle{Explained and non-explained variation}
\begin{center}
\tikzsetnextfilename{regression/variation_decomposition}
\mode<article>{\resizebox{0.7\textwidth}{!}{\input{img/regression/variation_decomposition}}}
\mode<presentation>{\resizebox{0.9\textwidth}{!}{\input{img/regression/variation_decomposition}}}
\end{center}
\end{frame}


\subsection{Correlation coefficients}
%---------------------------------------------------------------------slide----
\begin{frame}
\frametitle{Coefficient of determination}
From the residual variance is possible to define another correlation statistic easier to interpret.   
\begin{definition}[Sample coefficient of determination $r^2$]
Given a regression model  $y=f(x)$ of a two-dimensional variable $(X,Y)$, its \emph{coefficient of determination} is
\[
r^2 = 1- \frac{s_{ry}^2}{s_y^2}
\]
\end{definition}

As the residual variance ranges from 0 to $s_y^2$, we have
\[
\alert{0\leq r^2\leq 1}
\]

The greater $r^2$ is, the greater the goodness of fit of the regression model, and the more reliable will its predictions be. 
In particular, 
\begin{itemize}
\item If $r^2 =0$ then there is no relation as set by the regression model.
\item If $r^2=1$ then the relation set by the model is perfect. 
\end{itemize}
\end{frame}


%---------------------------------------------------------------------slide----
\begin{frame}
\frametitle{Linear coefficient of determination}
When the regression model is linear, the residual variance is
\begin{align*}
s_{ry}^2 & = \sum e_{ij}^2f_{ij} = \sum (y_j - f(x_i))^2f_{ij} = \sum \left(y_j - \bar y -\frac{s_{xy}}{s_x^2}(x_i-\bar x) \right)^2f_{ij}=\\
& = \sum \left((y_j - \bar y)^2 +\frac{s_{xy}^2}{s_x^4}(x_i-\bar x)^2 - 2\frac{s_{xy}}{s_x^2}(x_i-\bar x)(y_j -\bar y)\right)f_{ij} =\\
& = \sum (y_j - \bar y)^2f_{ij} +\frac{s_{xy}^2}{s_x^4}\sum (x_i-\bar x)^2f_{ij}- 2\frac{s_{xy}}{s_x^2}\sum (x_i-\bar x)(y_j -\bar y)f_{ij}=\\
& = s_y^2 + \frac{s_{xy}^2}{s_x^4}s_x^2 - 2 \frac{s_{xy}}{s_x^2}s_{xy} = s_y^2 - \frac{s_{xy}^2}{s_x^2}.
\end{align*}
and the coefficient of determination is  
\begin{align*}
r^2 &= 1- \frac{s_{ry}^2}{s_y^2} = 1- \frac{s_y^2 - \frac{s_{xy}^2}{s_x^2}}{s_y^2} = 1 - 1 + \frac{s_{xy}^2}{s_x^2s_y^2} = \frac{s_{xy}^2}{s_x^2s_y^2}.
\end{align*}
\end{frame}


%---------------------------------------------------------------------slide----
\begin{frame}
\frametitle{Linear coefficient of determination calculation}
\framesubtitle{Example of heights and weights}
In the sample of heights and weights, we had
\[
\begin{array}{lll}
\bar x = 174.67 \mbox{ cm} & \quad & s^2_x = 102.06 \mbox{ cm}^2\\
\bar y = 69.67 \mbox{ Kg} & & s^2_y = 164.42 \mbox{ Kg}^2\\
s_{xy} = 104.07 \mbox{ cm$\cdot$ Kg}
\end{array}
\]
Thus, the linear coefficient of determination is 
\[
r^2 = \frac{s_{xy}^2}{s_x^2s_y^2} = \frac{(104.07 \mbox{ cm$\cdot$Kg})^2}{102.06 \mbox{ cm}^2 \cdot 164.42 \mbox{ Kg}^2} = 0.65.
\]
This means that the linear model of weight on height explains the 65\% of the variation of weight, and the linear model of height on weight also explains 65\% of the variation of height. 
\end{frame}


%---------------------------------------------------------------------slide----
\begin{frame}
\frametitle{Correlation coefficient}
% Another common statistics to measure the linear association between two variables is the \emph{correlation
% coefficient}.
\begin{definition}[Sample correlation coefficient]
Given a sample of a two-dimensional variable $(X,Y)$, the \emph{sample correlation coefficient} is the square root of the linear coefficient of determination, with the sign of the covariance,
\[
r = \dfrac{s_{xy}}{s_xs_y}.
\]
\end{definition}
As $r^2$ ranges from 0 to 1, $r$  ranges from -1 to 1,
\[
\alert{-1\leq r\leq 1}
\]
The correlation coefficient measures not only the strength of the linear association but also its direction (increasing or decreasing):
\begin{itemize}
\item If $r=0$ then there is no linear relation.
\item Si $r=1$ then there is a perfect increasing linear relation.
\item Si $r=-1$ then there is a perfect decreasing linear relation.
\end{itemize}
\end{frame}


%---------------------------------------------------------------------slide----
\begin{frame}
\frametitle{Correlation coefficient}
\framesubtitle{The example of heights and weights}
In the sample of heights and weights, we had
\[
\begin{array}{lll}
\bar x = 174.67 \mbox{ cm} & \quad & s^2_x = 102.06 \mbox{ cm}^2\\
\bar y = 69.67 \mbox{ Kg} & & s^2_y = 164.42 \mbox{ Kg}^2\\
s_{xy} = 104.07 \mbox{ cm$\cdot$ Kg}
\end{array}
\]
Thus, the correlation coefficient is 
\[
r = \frac{s_{xy}}{s_xs_y} = \frac{104.07 \mbox{ cm$\cdot$Kg}}{10.1 \mbox{ cm} \cdot 12.82 \mbox{ Kg}} = +0.8.
\]
This means that there is a rather strong linear, increasing, relation between height and weight.
\end{frame}


%---------------------------------------------------------------------slide----
\begin{frame}
\frametitle{Different correlations}
\centering
\tikzsetnextfilename{regression/different_correlations}
\resizebox{\textwidth}{!}{\input{img/regression/different_correlations}}
\end{frame}


%---------------------------------------------------------------------slide----
\begin{frame}
\frametitle{Reliability of regression predictions}
The coefficient of determination explains the goodness of fit of a regression model, but there are other factors that influence the reliability of regression predictions:

\begin{itemize}
\item The coefficient of determination: The greater $r^2$, the greater the goodness of fit and the more reliable the predictions are.
\item The variability of the population distribution: The greater the variation, the more difficult to predict and the less reliable the predictions are.
\item The sample size: The greater the sample size, the more information we have and the more reliable the
predictions are. 
\end{itemize}

In addition, we have to take into account that a regression model is only valid for the range of values observed in the sample. 
That means that, as we don't have any information outside that range, we must not do predictions for values far from that range.
\end{frame}


\subsection{Non-linear regression}
%---------------------------------------------------------------------slide----
\begin{frame}
\frametitle{Non-linear regression}
The fit of a non-linear regression can be also done by the least square fitting method.

However, in some cases the fitting of a non-linear model can be reduced to the fitting of a linear model applying a simple transformation to the variables of the model. 
\end{frame}


%---------------------------------------------------------------------slide----
\begin{frame}
\frametitle{Transformations of non-linear regression models}
\begin{itemize}
\mode<presentation>{\small}
\item \highlight{Logarithmic model}: A logarithmic model $y = a+b \log x$ can be transformed in a linear model with
the change $t=\log x$:
\[y=a+b\log x = a+bt.\]
\item \highlight{Exponential model}: An exponential model $y = e^{a+bx}$ can be transformed in a linear model with the
change $z = \log y$:
\[z = \log y = \log(e^{a+bx}) =  a+bx. \]
\item \highlight{Potential model}: A potential model $y = ax^b$ can be transformed in a linear model with the changes
$t=\log x$ and $z=\log y$:
\[z = \log y = \log(ax^b) = \log a + b \log x = a^\prime+bt.\]
\item \highlight{Inverse model}: An inverse model $y = a+b/x$ can be transformed in a linear model with the change $t=1/x$:
\[y = a + b(1/x) = a+bt.\]
\item \highlight{Sigmoidal model}: A sigmoidal model $y = e^{a+b/x}$ can be transformed in a linear model with the
changes $t=1/x$ and $z=\log y$:
\[z = \log y = \log (e^{a+b/x}) = a+b(1/x) = a+bt.\]
\end{itemize}
\end{frame}


%---------------------------------------------------------------------slide----
\begin{frame}
\frametitle{Exponential relation}
\framesubtitle{Example of evolution of a bacterial culture}
The number of bacteria in a culture evolves with time according to the table below.
\begin{columns}
\begin{column}{0.3\textwidth}
\[
\begin{array}{c|c}
\mbox{Hours} & \mbox{Bacteria}\\
\hline
0 &  25 \\
1 & 28 \\
2 &  47\\
3 & 65 \\
4 & 86\\
5 & 121\\
6 & 190\\
7 & 290\\
8 & 362
\end{array}
\]
\end{column}
\begin{column}{0.6\textwidth}
The scatter plot of the sample is showed below.
\begin{center}
\tikzsetnextfilename{regression/bacteria_evolution}
\mode<article>{\resizebox{0.7\textwidth}{!}{\input{img/regression/bacteria_evolution}}}
\mode<presentation>{\resizebox{0.9\textwidth}{!}{\input{img/regression/bacteria_evolution}}}
\end{center}
\end{column}
\end{columns}
\end{frame}


%---------------------------------------------------------------------slide----
\begin{frame}
\frametitle{Exponential relation}
\framesubtitle{Example of evolution of a bacterial culture}
Fitting a linear model we get
\begin{columns}
\begin{column}{0.3\textwidth}
\[
\begin{array}{c|c}
\mbox{Hours} & \mbox{Bacteria}\\
\hline
0 &  25 \\
1 & 28 \\
2 &  47\\
3 & 65 \\
4 & 86\\
5 & 121\\
6 & 190\\
7 & 290\\
8 & 362
\end{array}
\]
\[
\mbox{Bacteria} = -30.18+41,27\,\mbox{Hours, with } r^2=0.85. 
\]
\end{column}
\begin{column}{0.6\textwidth}
\begin{center}
\tikzsetnextfilename{regression/linear_regression_bacteria}
\mode<article>{\resizebox{0.7\textwidth}{!}{\input{img/regression/linear_regression_bacteria}}}
\mode<presentation>{\resizebox{0.9\textwidth}{!}{\input{img/regression/linear_regression_bacteria}}}
\end{center}
\end{column}
\end{columns}
\begin{center}
\emph{Is a good model?}
\end{center}
\end{frame}


%---------------------------------------------------------------------slide----
\begin{frame}
\frametitle{Fitting an exponential regression model}
\framesubtitle{Example of evolution of a bacterial culture}
Although the linear model is not bad, according to the shape of the point cloud of the scatter plot, an exponential model looks more suitable. 

To construct an exponential model $y = e^{a+bx}$ we can apply the transformation $z=\log y$, that is, applying a logarithmic transformation to the dependent variable.
\begin{columns}
\begin{column}{0.45\textwidth}
\[
\begin{array}{c|c|c}
\mbox{Hours} & \mbox{Bacteria} & \mbox{$\log$(Bacteria)}\\
\hline
0 &  25 & 3.22\\
1 & 28 & 3.33\\
2 &  47 & 3.85\\
3 & 65  & 4.17\\
4 & 86 & 4.45\\
5 & 121 & 4.80\\
6 & 190 & 5.25\\
7 & 290 & 5.67\\
8 & 362 & 5.89
\end{array}
\]
\end{column}
\begin{column}{0.55\textwidth}
\begin{center}
\tikzsetnextfilename{regression/log_bacteria_evolution}
\mode<article>{\resizebox{0.7\textwidth}{!}{\input{img/regression/log_bacteria_evolution}}}
\mode<presentation>{\resizebox{0.9\textwidth}{!}{\input{img/regression/log_bacteria_evolution}}}
\end{center}
\end{column}
\end{columns}
\end{frame}


%---------------------------------------------------------------------slide----
\begin{frame}
\frametitle{Fitting an exponential regression model}
\framesubtitle{Example of evolution of a bacterial culture}
Now it only remains to compute the regression line of the logarithm of bacteria on hours,
\begin{columns}
\begin{column}{0.45\textwidth}
\[
\mbox{$\log$(Bacteria)} = 3.107 + 0.352\, \mbox{Horas},
\]
and, undoing the change of variable, 
\[
\mbox{Bacteria} = e^{3.107+0.352\,\mbox{Hours}},
\]
with $r^2=0.99$.

Thus, the exponential model fits much better than the linear model. 
\end{column}
\begin{column}{0.55\textwidth}
\begin{center}
\tikzsetnextfilename{regression/exponential_regression_bacteria}
\mode<article>{\resizebox{0.7\textwidth}{!}{\input{img/regression/exponential_regression_bacteria}}}
\mode<presentation>{\resizebox{0.9\textwidth}{!}{\input{img/regression/exponential_regression_bacteria}}}
\end{center}
\end{column}
\end{columns}
\end{frame}

\subsection{Regression risks}

%---------------------------------------------------------------------slide----
\begin{frame}
\frametitle{Lack of fit doesn't mean independence}
It is important to note that every regression model has its own coefficient of determination. Thus, a coefficient of determination near zero means that there is no relation as set by the model, but \emph{that does not mean that the variables are independent}, because there could be a different type of relation.
\begin{center}
\tikzsetnextfilename{regression/linear_regression_parabolic_relation}
\resizebox{0.49\textwidth}{!}{\input{img/regression/linear_regression_parabolic_relation}}
\tikzsetnextfilename{regression/parabolic_regression}
\resizebox{0.49\textwidth}{!}{\input{img/regression/parabolic_regression}}
\end{center}
\end{frame}


%---------------------------------------------------------------------slide----
\begin{frame}
\frametitle{Outliers in regression}
Outliers in regression studies are points that clearly do not follow the tendency of the rest of points, even if the values of the pair are not outliers for every variable separately.
\begin{center}
\tikzsetnextfilename{regression/scatterplot_with_outliers}
\resizebox{0.7\textwidth}{!}{\input{img/regression/scatterplot_with_outliers}}
\end{center}
\end{frame}


%---------------------------------------------------------------------slide----
\begin{frame}
\frametitle{Outliers influence in regression}
Outliers in regression studies can provoke drastic changes in the regression models. 
\begin{center}
\tikzsetnextfilename{regression/linear_regression_with_outliers}
\resizebox{0.49\textwidth}{!}{\input{img/regression/linear_regression_with_outliers}}
\tikzsetnextfilename{regression/linear_regression_without_outliers}
\resizebox{0.49\textwidth}{!}{\input{img/regression/linear_regression_without_outliers}}
\end{center}
\end{frame}


%---------------------------------------------------------------------slide----
\begin{frame}
\frametitle{Simpson's paradox}
Sometimes a trend can disappears or even reverses when we split the sample into groups according to a qualitative variable that is related to the dependent variable.
This is known as the \emph{Simpson's paradox}. 
\begin{center}
\tikzsetnextfilename{regression/simpson_paradox_1}
\resizebox{0.7\textwidth}{!}{\input{img/regression/simpson_paradox_1}}
\end{center}
\end{frame}


%---------------------------------------------------------------------slide----

\begin{frame}
\frametitle{Simpson's paradox}
Sometimes a trend can disappears or even reverses when we split the sample into groups according to a qualitative variable that is related to the dependent variable.
This is known as the \emph{Simpson's paradox}. 
\begin{center}
\tikzsetnextfilename{regression/simpson_paradox_2}
\resizebox{0.7\textwidth}{!}{\input{img/regression/simpson_paradox_2}}
\end{center}
\end{frame}


% \subsection{Medidas de relación entre atributos}
% %---------------------------------------------------------------------slide----
% \begin{frame}
% \frametitle{Relaciones entre atributos}
% Los modelos de regresión vistos sólo pueden aplicarse cuando las variables estudiadas son cuantitativas.
% 
% Cuando se desea estudiar la relación entre atributos, tanto ordinales como nominales, es necesario recurrir a otro tipo
% de medidas de relación o de asociación. En este tema veremos tres de ellas:
% \begin{itemize}
% \item Coeficiente de correlación de Spearman.
% \item Coeficiente chi-cuadrado.
% \item Coeficiente de contingencia.
% \end{itemize}
% 
% \note{La regresión sólo tiene sentido cuando cuando las variables estudiadas son cuantitativas.
% 
% Cuando se desea estudiar la relación entre atributos, tanto ordinales como nominales, es necesario recurrir a otro tipo
% de medidas de relación o de asociación. Veremos tres de las más importantes que son
% \begin{itemize}
% \item Coeficiente de correlación de Spearman.
% \item Coeficiente chi-cuadrado.
% \item Coeficiente de contingencia.
% \end{itemize}
% }
% \end{frame}
% 
% 
% %---------------------------------------------------------------------slide----
% \begin{frame}
% \frametitle{Coeficiente de correlación de Spearman}
% Cuando se tengan atributos ordinales es posible ordenar sus categorías y asignarles valores ordinales, de manera que se
% puede calcular el coeficiente de correlación lineal entre estos valores ordinales. 
% 
% Esta medida de relación entre el orden que ocupan las categorías de dos atributos ordinales se conoce como coeficiente
% ce correlación de Spearman, y puede demostrarse fácilmente que puede calcularse a partir de la siguiente fórmula
% 
% \begin{definicion}[Coeficiente de correlación de Spearman]
% Dada una muestra de $n$ individuos en los que se han medido dos atributos ordinales $X$ e $Y$, el coeficiente de
% correlación de Spearman se define como: 
% \[
% r_s = 1-\frac{6\sum d_i^2}{n(n^2-1)}
% \]
% donde $d_i$ es la diferencia entre el valor ordinal de $X$ y el valor ordinal de $Y$ del individuo $i$.
% \end{definicion}
% 
% \note{Si queremos estudiar la relación entre dos atributos ordinales, como sus categorías pueden ordenarse, una posibilidad tomar sus
% valores de orden, que serían numéricos, y calcular el coeficiente de correlación lineal entre los órdenes.
% 
% Esta medida se conoce como coeficiente de correlación de Spearman, que se representa como $r_s$ y también puede calcularse haciendo la suma
% de los cuadrados de las diferencias entre los números de orden del valor de $X$ e $Y$ en cada indiviudo, multiplicada por 6, dividida por el
% tamaño de la muestra multiplicado por el cuadrado del tamaño de la muestra menos 1 y restando el resultado del cociente a 1.
% }
% \end{frame}
% 
% 
% %---------------------------------------------------------------------slide----
% \begin{frame}
% \frametitle{Interpretación del coeficiente de correlación de Spearman}
% Como el coeficiente de correlación de Spearman es en el fondo el coeficiente de correlación lineal aplicado a los
% órdenes, se tiene:
% \[
% -1\leq r_s\leq 1,
% \]
% de manera que:
% \begin{itemize}
% \item Si $r_s=0$ entonces no existe relación entre los atributos ordinales.
% \item Si $r_s=1$ entonces los órdenes de los atributos coinciden y existe una relación directa perfercta.
% \item Si $r_s=-1$ entonces los órdenes de los atributos están invertidos y existe una relación inversa perfecta.
% \end{itemize}
% En general, cuanto más cerca de $1$ o $-1$ esté $r_s$, mayor será la relación entre los atributos, y cuanto más cerca
% de $0$, menor será la relación.
% 
% \note{Como el coeficiente de correlación de Spearman es en el fondo el coeficiente de correlación lineal aplicado a los
% órdenes, su valor estará entre -1 y 1 y se interpreta de manera similar al coeficiente de correlación lineal, es decir,
% \begin{itemize}
% \item Si $r_s=0$ entonces no existe relación entre los atributos ordinales.
% \item Si $r_s=1$ entonces los órdenes de los atributos coinciden y existe una relación directa perfercta.
% \item Si $r_s=-1$ entonces los órdenes de los atributos están invertidos y existe una relación inversa perfecta.
% \end{itemize}
% }
% \end{frame}
% 
% 
% %---------------------------------------------------------------------slide----
% \begin{frame}
% \frametitle{Cálculo del coeficiente de correlación de Spearman}
% \framesubtitle{Ejemplo}
% Una muestra de 5 alumnos realizaron dos tareas diferentes $X$ e $Y$, y se ordenaron de acuerdo a la destreza que
% manifestaron en cada tarea:
% \[
% \begin{array}{lrrrr}
% \hline
% \text{Alumnos} & X & Y & d_i & d_i^2\\
% \hline
% \text{Alumno 1} & 2 & 3 & -1 & 1\\
% \text{Alumno 2} & 5 & 4 & 1 & 1 \\
% \text{Alumno 3} & 1 & 2 & -1 & 1\\
% \text{Alumno 4} & 3 & 1 & 2 & 4\\
% \text{Alumno 5} & 4 & 5 & -1 & 1\\
% \hline
% \sum &  &  & 0 & 8 \\
% \hline
% \end{array}
% \]
% El coeficiente de correlación de Spearman para esta muestra es 
% \[
% r_s = 1-\frac{6\sum d_i^2}{n(n^2-1)} = 1- \frac{6\cdot 8}{5(5^2-1)} = 0.6,
% \]
% lo que indica que existe bastante relación directa entre las destrezas manifestadas en ambas tareas.
% 
% \note{Para ilustrar el cálculo del coeficiente de correlación de Spearman, supongamos una muestra de 5 alumnos que realizaron dos tareas
% diferentes $X$ e $Y$, y se ordenaron de acuerdo a la destreza que manifestaron en cada tarea.
% 
% El primer alumno fue el segundo con más destreza en la tarea $X$ y el tercero en la tarea $Y$ de manera que la diferencia entre los órdenes
% en $X$ e $Y$ es $-1$ y su cuadrado vale 1. El segundo alumno fue el peor en la tarea $X$ y el penúltimo en la tarea $Y$ de manera que la
% diferencia entre sus órdenes en $X$ e $Y$ vale también $-1$ y su cuadrado 1, etc.  
% 
% Una vez calculados los cuadrados de las diferencias entre los órdenes, se suman, se multiplican por 6 y se dividen por el cuadrado del
% tamaño de la muestra, que vale 5,  por el cuadrado del tamaño de la muestra menos, que es $5^2-1$ y finalmente el resultado del cociente se
% resta a 1. Esto nos da un coeficiente de correlación de Spearman de 0.6, lo cual indica que existe bastante relación directa entre las
% destrezas manifestadas en ambas tareas.
% }
% \end{frame}
% 
% 
% %---------------------------------------------------------------------slide----
% \begin{frame}
% \frametitle{Cálculo del coeficiente de correlación de Spearman}
% \framesubtitle{Ejemplo con empates}
% Cuando hay empates en el orden de las categorías se atribuye a cada valor empatado la media aritmética de los valores
% ordinales que hubieran ocupado esos individuos en caso de no haber estado empatados.
% 
% Si en el ejemplo anterior los alumnos 4 y 5 se hubiesen comportado igual en la primera tarea y los alumnos 3 y 4 se
% hubiesen comportado igual en la segunda tarea, entonces se tendría
% \[
% \begin{array}{lrrrr}
% \hline
% \text{Alumnos} & X & Y & d_i & d_i^2\\
% \hline
% \text{Alumno 1} & 2 & 3 & -1 & 1\\
% \text{Alumno 2} & 5 & 4 & 1 & 1 \\
% \text{Alumno 3} & 1 & 1.5 & -0.5 & 0.25\\
% \text{Alumno 4} & 3.5 & 1.5 & 2 & 4\\
% \text{Alumno 5} & 3.5 & 5 & -1.5 & 2.25\\
% \hline
% \sum &  &  & 0 & 8.5 \\
% \hline
% \end{array}
% \]
% El coeficiente de correlación de Spearman para esta muestra es 
% \[
% r_s = 1-\frac{6\sum d_i^2}{n(n^2-1)} = 1- \frac{6\cdot 8.5}{5(5^2-1)} = 0.58.
% \]
% 
% \note{Cuando haya individuos con la misma categoría del atributo, entonces dichos individuos recibirán el mismo número de orden y de manera
% que habrá empates en el orden. En tal caso se atribuye a cada valor empatado la media aritmética de los valores ordinales que hubieran
% ocupado esos individuos en caso de no haber estado empatados.
% 
% Si en ejemplo anterior los alumnos 4 y 5 se hubiesen comportado igual en la primera tarea, entonces como ambos ocupan la posición 3 y 4
% en la ordenación, se toma la media $3.5$ y se le asigna dicho valor a ambos. Del mismo modo, si los alumnos 3 y 4 se hubiesen comportado
% igual en la segunda tarea, como el orden que ocupan es el primero y el segundo, entonces a ambos se le asigna la media $1.5$.
% 
% El resto de los cálculos para el coeficiente de Spearman se haría igual que en el caso en que no había empates.
% }
% \end{frame}
% 
% 
% %---------------------------------------------------------------------slide----
% \begin{frame}
% \frametitle{Relación entre atributos nominales}
% Cuando se quiere estudiar la relación entre atributos nominales no tiene sentido calcular el coeficiente de correlación
% de Spearman ya que las categorías no pueden ordenarse.
% 
% Para estudiar la relación entre atributos nominales se utilizan medidas basadas en las frecuencias de la tabla de
% frecuencias bidimensional, que para atributos se suele llamar \emph{tabla de contingencia}.
% 
% \highlight{Ejemplo}} En un estudio para ver si existe relación entre el sexo y el hábito de fumar se ha tomado
% una muestra de 100 personas.
% La tabla de contingencia resultante es
% \[
% \begin{array}{|l|rr|r|} 
% \hline
% \text{Sexo}\backslash\text{Fuma} & \text{Si} & \text{No} & n_i\\
% \hline
% \text{Mujer} & 12 & 28 & 40 \\
% \text{Hombre} & 26 & 34 & 60 \\
% \hline
% n_j & 38 & 62 & 100\\
% \hline
% \end{array}
% \]
% Si el hábito de fumar fuese independiente del sexo, la proporción de fumadores en mujeres y hombres sería la misma.
% 
% \note{Cuando se quiere estudiar la relación entre atributos nominales no tiene sentido calcular el coeficiente de correlación
% de Spearman ya que las categorías no pueden ordenarse. En este caso se utilizan medidas basadas en las frecuencias de la tabla de
% frecuencias bidimensional, que para atributos se suele llamar \emph{tabla de contingencia}.
% 
% En este ejemplo tenemos una tabla de contingencia para los atributos sexo y hábito de fumar, ambos nominales. En la muestra hay 12 mujeres
% fumadoras, 28 no fumadoras, 26 hombres fumadores y 34 no fumadores. En total tenemos una muestra de tamaño 100. 
% 
% La forma de estudiar la relación entre el sexo y el hábito de fumar es por medio de las frecuencias relativas de cada par de categorías.
% Así, si el hábito de fumar fuese independiente del sexo, la proporción de fumadores en mujeres y hombres sería la misma, mientras que si
% hubiese dependencia, serían significativamente diferentes.
% }
% \end{frame}
% 
% 
% %---------------------------------------------------------------------slide----
% \begin{frame}
% \frametitle{Frecuencias teóricas o esperadas}
% En general, dada una tabla de contingencia para dos atributos $X$ e $Y$, 
% \[
% \begin{array}{|c|ccccc|c|}
% \hline
% X\backslash Y & y_1 & \cdots & y_j & \cdots & y_q & n_x\\
% \hline
% x_1 & n_{11} & \cdots & n_{1j} & \cdots & n_{1q} & n_{x_1}\\
% \vdots & \vdots & \ddots & \vdots & \ddots & \vdots & \vdots \\
% x_i & n_{i1} & \cdots & n_{ij} & \cdots & n_{iq} & n_{x_i}\\
% \vdots & \vdots & \ddots & \vdots & \ddots & \vdots & \vdots\\
% x_p & n_{p1} & \cdots & n_{pj} & \cdots & n_{pq} & n_{x_p} \\
% \hline
% n_y & n_{y_1} & \cdots & n_{y_j} & \cdots & n_{y_q} & n\\
% \hline
% \end{array}
% \]
% si $X$ e $Y$ fuesen independientes, para cualquier valor $y_j$ se tendría
% \[
% \frac{n_{1j}}{n_{x_1}} = \frac{n_{2j}}{n_{x_2}} = \cdots = \frac{n_{pj}}{n_{x_p}} = \frac{n_{1j}+\cdots
% +n_{pj}}{n_{x_1}+\cdots+n_{x_p}} = \frac{n_{y_j}}{n},
% \]
% de donde se deduce que
% \[
% n_{ij} = \frac{n_{x_i}n_{y_j}}{n}.
% \]
% 
% A esta última expresión se le llama \emph{frecuencia teórica} o \emph{frecuencia esperada} del par $(x_i,y_j)$. 
% 
% \note{En general, dada una tabla de contingencia pra dos atributos $X$ e $Y$, si las variables fuesen independientes, entonces la proporción
% de individuos que presentan el valor $y_j$ en la variable $Y$ entre los que presentan el valor $x_1$ en la variable $X$ sería la frecuencia
% absoluta del par $(x_i,y_j)$ $n_{ij}$ divida entre la frecuencia absoluta de $x_1$ y esta frecuencia sería la misma que la proporción de
% individuos con el valor $y_j$ en $Y$ entre los que presentan el valor $x_2$ en $X$, y en general, al ser independiente de $X$ sería la misma
% para cualquier otra categoría de $X$, de manera que todas serían iguales a la proporción total de individuos en la muestra con valor $y_j$
% en la variable $Y$, que se obtendría dividiendo la frecuencia absoluta de $y_j$ entre el tamaño de la muestra.
% 
% De esta igualdad se puede deducir que la frecuencia absoluta de cualquier par $(x_i,y_j)$ se podría calcular multiplicando las frecuencias
% absolutas de $x_i$, $n_i$ y $y_j$ que es $n_j$ y dividendo por el tamaño de la muestra.
% 
% A estas frecuencias calculadas bajo la hipótesis de independencia, se les llama \emph{frecuencia teóricas} o \emph{frecuencia esperadas}.
% }
% \end{frame}
% 
% 
% %---------------------------------------------------------------------slide----
% \begin{frame}
% \frametitle{Coeficiente chi-cuadrado $\chi^2$}
% Es posible estudiar la relación entre dos atributos $X$ e $Y$ comparando las frecuencias reales con las esperadas:
% \begin{definicion}[Coeficiente chi-cuadrado $\chi^2$]
% Dada una muestra de tamaño $n$ en la que se han medido dos atributos $X$ e $Y$, se define el coeficiente $\chi^2$ como
% \[
% \chi^2 = \sum_{i=1}^p\sum_{j=1}^q \frac{\left(n_{ij}-\frac{n_{x_i}n_{y_j}}{n}\right)^2}{\frac{n_{x_i}n_{y_j}}{n}},
% \]
% donde $p$ es el número de categorías de $X$ y $q$ el número de categorías de $Y$.
% \end{definicion}
% 
% Por ser suma de cuadrados, se cumple que 
% \[
% \chi^2 \geq 0,
% \]
% de manera que $\chi^2=0$ cuando los atributos son independientes, y crece a medida que aumenta la dependencia entre las variables.
% 
% \note{Una posible forma de estudiar la relación entre dos atributos nominales es comparar sus frecuencias absolutas reales con las esperadas
% bajo la hipótesis de indepencia. Esto es precisamente lo que hace un estadístico conocido como coeficiente chi-cuadrado, que se denota con
% la letra griega $\chi^2$ y se calcula sumando para cada par de la variable bidimensional los cuadrados de las diferencias entre las
% frecuencias reales y las frecuencias esperadas, dividos por las propias frecuencias esperadas.
% 
% Como se trata de una suma de cuadrados, el coeficiente chi-cuadrado siempre es positivo. Cuando los atributos sean independientes, las
% frecuencias reales coincidirán con las esperadas y todos los sumandos se anularán, de manera que el coeficiente chi-cuadrado valdrá cero. Y
% en la medida que cada vez haya más dependencia, las diferencias entre las frecuencias reales y las esperadas serán cada vez mayores de forma
% que el coeficiente chi-cuadrado será también mayor.
% }
% \end{frame}
% 
% 
% %---------------------------------------------------------------------slide----
% \begin{frame}
% \frametitle{Cálculo del coeficiente chi-cuadrado $\chi^2$}
% \framesubtitle{Ejemplo}
% Siguiendo con el ejemplo anterior, a partir de la tabla de contingencia 
% \[
% \begin{array}{|l|rr|r|} 
% \hline
% \text{Sexo}\backslash\text{Fuma} & \text{Si} & \text{No} & n_i\\
% \hline
% \text{Mujer} & 12 & 28 & 40 \\
% \text{Hombre} & 26 & 34 & 60 \\
% \hline
% n_j & 38 & 62 & 100\\
% \hline
% \end{array}
% \]
% se obtienen las siguientes frecuencias esperadas:
% \[
% \renewcommand{\arraystretch}{1.5}
% \begin{array}{|l|rr|r|} 
% \hline
% \text{Sexo\Fuma} & \text{Si} & \text{No} & n_i\\
% \hline
% \text{Mujer} & \frac{40\cdot 38}{100}=15.2 & \frac{40\cdot 62}{100}=24.8 & 40 \\
% \text{Hombre} & \frac{60\cdot 38}{100}=22.8 & \frac{60\cdot 62}{100}=37.2 & 60 \\
% \hline
% n_j & 38 & 62 & 100\\
% \hline
% \end{array}
% \]
% y el coeficiente $\chi^2$ vale
% \[
% \chi^2 = \frac{(12-15.2)^2}{15.2}+\frac{(28-24.8)^2}{24.8}+\frac{(26-22.8)^2}{22.8}+\frac{(34-37.2)^2}{37.2} = 1.81,
% \]
% lo que indica que no existe gran relación entre el sexo y el hábito de fumar.
% 
% \note{Siguiendo con el ejemplo del sexo y el hábito de fumar, a partir de la tabla de contingencia se calculan las frecuencias esperadas de
% cada par. La frecuencia esperada de mujeres fumadoras es el número de mujeres 40, por el número de personas fumadoras 38, dividido por el
% tamaño de la muestra 100, lo que da $15.2$. Etc. 
% 
% Una vez calculadas las frecuencias esperadas, para calcular el coeficiente chi-cuadrado vamos calculando los cuadrados de las diferencias
% entre las frecuencias reales y las esperadas, es decir, para las mujeres fumadoras sería $(12-15.2)^2$ y esto se divide por la frecuencia
% esperada $15.2$. Del mismo modo se calculan los sumandos para el resto de los pares y se realiza la suma, lo que nos da un coeficiente
% chi-cuadrado de $1.81$, que al ser un valor próximo a 0 indica que no hay gran relación entre el sexo y el hábito de fumar.
% }
% \end{frame}
% 
% 
% %---------------------------------------------------------------------slide----
% \begin{frame}
% \frametitle{Coeficiente de contingencia}
% El coeficiente $\chi^2$ depende del tamaño muestral, ya que al multiplicar por una constante las frecuencias de todas
% las casillas, su valor queda multiplicado por dicha constante, lo que podría llevarnos al equívoco de pensar que ha
% aumentado la relación, incluso cuando las proporciones se mantienen.
% En consecuencia el valor de $\chi^2$ no está acotado superiormente y resulta difícil de interpretar.
% 
% Para evitar estos problemas se suele utilizar el siguiente estadístico:
% \begin{definicion}[Coeficiente de contingencia]
% Dada una muestra de tamaño $n$ en la que se han medido dos atributos $X$ e $Y$, se define el \emph{coeficiente de
% contingencia} como 
% \[
% C = \sqrt{\frac{\chi^2}{\chi^2+n}}
% \]
% \end{definicion}
% 
% \note{El principal problema del coeficiente chi-cuadrado es que depende del tamaño muestral ya que se calcula a partir de las frecuencias
% absolutas, no de las relativas. De hecho, se puede comprobar que al multiplicar por una constante las frecuencias de todas
% las casillas de la tabla de contingencia, su valor queda multiplicado por dicha constante, lo que podría llevarnos al equívoco de pensar que
% ha aumentado la relación, incluso cuando las proporciones se mantienen. En consecuencia el valor de $\chi^2$ no está acotado superiormente y
% resulta difícil de interpretar.
% 
% Para evitar estos problemas suele utilizarse otro estadístico conocido como coeficiente de contingencia, que se denota $C$, y se calcula a
% partir del coeficiente chi-cuadrado haciendo la raíz del cociente entre el coeficiente-chi cuadrado entre el mismo coeficiente chi-cuadrado
% mas el tamaño de la muestra.
% }
% \end{frame}
% 
% 
% %---------------------------------------------------------------------slide----
% \begin{frame}
% \frametitle{Interpretación del coeficiente de contingencia}
% De la definición anterior se deduce que 
% \[
% 0\leq C\leq 1,
% \]
% de manera que cuando $C=0$ las variables son independientes, y crece a medida que aumenta la relación.
% 
% Aunque $C$ nunca puede llegar a valer 1, se puede demostrar que para tablas de contingencia con $k$ filas y $k$
% columnas, el valor máximo que puede alcanzar $C$ es $\sqrt{(k-1)/k}$.
% 
% \highlight{Ejemplo}} En el ejemplo anterior el coeficiente de contingencia vale
% \[
% C = \sqrt{\frac{1.81}{1.81+100}} = 0.13.
% \]
% Como se trata de una tabla de contingencia de $2\times 2$, el valor máximo que podría tomar el coeficiente de
% contingencia es $\sqrt{(2-1)/2}=\sqrt{1/2}=0.707$, y como $0.13$ está bastante lejos de este valor, se puede concluir
% que no existe demasiada relación entre el hábito de fumar y el sexo.
% 
% \note{De la definición del coeficiente de contingencia es fácil ver que, al ser el numerador siempre menor que el denominador, siempre
% estará entre 0 y 1, de manera que el coeficiente de contingencia será nulo cuando las variables sean independientes, y estará más próximo a
% 1 a medida que aumente la dependencia.
% 
% Aunque el coeficiente de contingencia nunca puede llegar a valer 1, se puede demostrar que para tablas de contingencia con $k$ filas y $k$
% columnas, el valor máximo que puede alcanzar es $\sqrt{(k-1)/k}$, de modo que se puede comparar con este valor.
% 
% En el ejemplo del sexo y el hábito de fumar, como el coeficiente chi-cuadrado valía $1.81$, el coeficiente de contingencia vale la
% raíz cuadrada de $1.81$ dividido entre $1.81$ más el tamaño de la muestra 100, lo que da $0.13$. Y como se trata de una tabla de
% contingencia de $2\times 2$, el valor máximo que podría tomar el coeficiente de contingencia es $\sqrt{(2-1)/2}=\sqrt{1/2}=0.707$, y como
% $0.13$ está bastante lejos de este valor, se puede concluir que no existe demasiada relación entre el hábito de fumar y el sexo.
% }
% \end{frame}
